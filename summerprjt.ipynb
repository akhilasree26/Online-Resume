{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1360,
          "sourceType": "datasetVersion",
          "datasetId": 732
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "summerprjt",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhilasree26/Online-Resume/blob/main/summerprjt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "usgs_earthquake_database_path = kagglehub.dataset_download('usgs/earthquake-database')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "IFT4IVdVdoXC",
        "outputId": "7a5e3c1b-9d8a-4064-a93d-7be7995d64ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/usgs/earthquake-database?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 590k/590k [00:00<00:00, 18.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T09:51:53.173588Z",
          "iopub.execute_input": "2025-08-01T09:51:53.173856Z",
          "iopub.status.idle": "2025-08-01T09:51:53.178448Z",
          "shell.execute_reply.started": "2025-08-01T09:51:53.173837Z",
          "shell.execute_reply": "2025-08-01T09:51:53.177705Z"
        },
        "id": "K8orChRGdoXH"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/kaggle/input/earthquake-database/database.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T09:51:53.179753Z",
          "iopub.execute_input": "2025-08-01T09:51:53.180466Z",
          "iopub.status.idle": "2025-08-01T09:51:53.276832Z",
          "shell.execute_reply.started": "2025-08-01T09:51:53.180445Z",
          "shell.execute_reply": "2025-08-01T09:51:53.275908Z"
        },
        "id": "gLpwMG6QdoXH",
        "outputId": "910792e9-5191-40ad-ed75-b61d51242bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/earthquake-database/database.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1938524384.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/earthquake-database/database.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/earthquake-database/database.csv'"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T09:52:39.77508Z",
          "iopub.execute_input": "2025-08-01T09:52:39.775399Z",
          "iopub.status.idle": "2025-08-01T09:52:39.830959Z",
          "shell.execute_reply.started": "2025-08-01T09:52:39.775374Z",
          "shell.execute_reply": "2025-08-01T09:52:39.830079Z"
        },
        "id": "XSizcMIndoXI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T09:58:44.154282Z",
          "iopub.execute_input": "2025-08-01T09:58:44.154601Z",
          "iopub.status.idle": "2025-08-01T09:58:44.192803Z",
          "shell.execute_reply.started": "2025-08-01T09:58:44.154577Z",
          "shell.execute_reply": "2025-08-01T09:58:44.19182Z"
        },
        "id": "E89zM5jDdoXI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "gP_yPi6sdoXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at missing values\n",
        "data.isna()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:00:06.969527Z",
          "iopub.execute_input": "2025-08-01T10:00:06.969843Z",
          "iopub.status.idle": "2025-08-01T10:00:07.009024Z",
          "shell.execute_reply.started": "2025-08-01T10:00:06.969821Z",
          "shell.execute_reply": "2025-08-01T10:00:07.008239Z"
        },
        "id": "8noMCs7mdoXL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#no. of missing values in each coloumn\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:00:44.251161Z",
          "iopub.execute_input": "2025-08-01T10:00:44.251564Z",
          "iopub.status.idle": "2025-08-01T10:00:44.272454Z",
          "shell.execute_reply.started": "2025-08-01T10:00:44.251531Z",
          "shell.execute_reply": "2025-08-01T10:00:44.271628Z"
        },
        "id": "_FlWr3radoXL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#taking it as mean to see in %\n",
        "data.isna().mean()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:03:01.330774Z",
          "iopub.execute_input": "2025-08-01T10:03:01.331737Z",
          "iopub.status.idle": "2025-08-01T10:03:01.353315Z",
          "shell.execute_reply.started": "2025-08-01T10:03:01.331707Z",
          "shell.execute_reply": "2025-08-01T10:03:01.352569Z"
        },
        "id": "wqBf2QpTdoXL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()>0.66*data.shape[0]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:05:15.114825Z",
          "iopub.execute_input": "2025-08-01T10:05:15.115227Z",
          "iopub.status.idle": "2025-08-01T10:05:15.135617Z",
          "shell.execute_reply.started": "2025-08-01T10:05:15.115202Z",
          "shell.execute_reply": "2025-08-01T10:05:15.134847Z"
        },
        "id": "5udAM3WkdoXM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[:,data.isna().sum()>0.66*data.shape[0]]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:07:43.064493Z",
          "iopub.execute_input": "2025-08-01T10:07:43.065173Z",
          "iopub.status.idle": "2025-08-01T10:07:43.097802Z",
          "shell.execute_reply.started": "2025-08-01T10:07:43.065145Z",
          "shell.execute_reply": "2025-08-01T10:07:43.096946Z"
        },
        "id": "wrCV2mUjdoXM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[:,data.isna().sum()>0.66*data.shape[0]].columns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:09:04.977275Z",
          "iopub.execute_input": "2025-08-01T10:09:04.977586Z",
          "iopub.status.idle": "2025-08-01T10:09:04.998504Z",
          "shell.execute_reply.started": "2025-08-01T10:09:04.977563Z",
          "shell.execute_reply": "2025-08-01T10:09:04.997582Z"
        },
        "id": "qStgO8jYdoXM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "null_columns=data.loc[:,data.isna().sum()>0.66*data.shape[0]].columns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:09:53.275737Z",
          "iopub.execute_input": "2025-08-01T10:09:53.276853Z",
          "iopub.status.idle": "2025-08-01T10:09:53.296735Z",
          "shell.execute_reply.started": "2025-08-01T10:09:53.276821Z",
          "shell.execute_reply": "2025-08-01T10:09:53.296024Z"
        },
        "id": "z_Jcu-tsdoXM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data= data.drop(null_columns,axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "EiqQTMkEdoXN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:11:05.206465Z",
          "iopub.execute_input": "2025-08-01T10:11:05.206774Z",
          "iopub.status.idle": "2025-08-01T10:11:05.226438Z",
          "shell.execute_reply.started": "2025-08-01T10:11:05.206752Z",
          "shell.execute_reply": "2025-08-01T10:11:05.225455Z"
        },
        "id": "CGdSAMZxdoXN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data['Root Mean Square']=data['Root Mean Square'].fillna(data['Root Mean Square'].mean())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:16:04.914445Z",
          "iopub.execute_input": "2025-08-01T10:16:04.91474Z",
          "iopub.status.idle": "2025-08-01T10:16:04.920806Z",
          "shell.execute_reply.started": "2025-08-01T10:16:04.914721Z",
          "shell.execute_reply": "2025-08-01T10:16:04.920042Z"
        },
        "id": "7IzJ15GqdoXN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.dropna(axis=0).reset_index(drop=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:16:09.32237Z",
          "iopub.execute_input": "2025-08-01T10:16:09.322645Z",
          "iopub.status.idle": "2025-08-01T10:16:09.346952Z",
          "shell.execute_reply.started": "2025-08-01T10:16:09.322627Z",
          "shell.execute_reply": "2025-08-01T10:16:09.346079Z"
        },
        "id": "U3dcihaTdoXN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T10:16:13.933372Z",
          "iopub.execute_input": "2025-08-01T10:16:13.933654Z",
          "iopub.status.idle": "2025-08-01T10:16:13.952361Z",
          "shell.execute_reply.started": "2025-08-01T10:16:13.933634Z",
          "shell.execute_reply": "2025-08-01T10:16:13.9516Z"
        },
        "id": "IqeouJDldoXN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Load dataset (try kaggles path; else assume 'data' exists in kernel)\n",
        "csv_path = '/kaggle/input/earthquake-database/database.csv'\n",
        "if os.path.exists(csv_path):\n",
        "    data = pd.read_csv(csv_path)\n",
        "    print(\"Loaded CSV from\", csv_path)\n",
        "else:\n",
        "    if 'data' in globals():\n",
        "        print(\"Using existing 'data' variable from kernel.\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Could not find dataset at {csv_path} and variable 'data' not in globals().\")\n",
        "\n",
        "# -------------------------\n",
        "# Minimal feature selection and cleaning\n",
        "if 'Datetime' not in data.columns:\n",
        "    if 'Date' in data.columns and 'Time' in data.columns:\n",
        "        data['Datetime'] = pd.to_datetime(data['Date'].astype(str) + ' ' + data['Time'].astype(str), errors='coerce')\n",
        "    elif 'time' in data.columns:\n",
        "        data['Datetime'] = pd.to_datetime(data['time'], errors='coerce')\n",
        "    else:\n",
        "        data['Datetime'] = pd.to_datetime(pd.Series(range(len(data))), errors='coerce')\n",
        "\n",
        "keep_cols = [c for c in ['Latitude','Longitude','Depth','Magnitude','Datetime'] if c in data.columns]\n",
        "df = data[keep_cols].dropna().copy()\n",
        "df = df.sort_values('Datetime').reset_index(drop=True)\n",
        "\n",
        "print(\"Final dataframe shape (after selecting essential columns):\", df.shape)\n",
        "print(\"Columns used:\", df.columns.tolist())\n",
        "\n",
        "# -------------------------\n",
        "# Scaling\n",
        "feature_cols = ['Latitude','Longitude','Depth']\n",
        "target_col = 'Magnitude'\n",
        "\n",
        "feature_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "\n",
        "features = df[feature_cols].values.astype(np.float32)\n",
        "target = df[[target_col]].values.astype(np.float32)\n",
        "\n",
        "features_scaled = feature_scaler.fit_transform(features)\n",
        "target_scaled = target_scaler.fit_transform(target)\n",
        "\n",
        "# -------------------------\n",
        "# Create supervised sequences\n",
        "def create_feature_sequences(X_feat, y_target, seq_len=20):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X_feat) - seq_len):\n",
        "        Xs.append(X_feat[i:i+seq_len])\n",
        "        ys.append(y_target[i+seq_len])\n",
        "    return np.array(Xs), np.array(ys).reshape(-1,1)\n",
        "\n",
        "SEQ_LEN = 20\n",
        "X, y = create_feature_sequences(features_scaled, target_scaled, seq_len=SEQ_LEN)\n",
        "print(\"Sequence data shape:\", X.shape, y.shape)\n",
        "\n",
        "# -------------------------\n",
        "# Train-test split\n",
        "test_size = 0.2\n",
        "split_idx = int((1 - test_size) * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(\"Train samples:\", X_train.shape[0], \"Test samples:\", X_test.shape[0])\n",
        "\n",
        "# -------------------------\n",
        "# CNN-LSTM model\n",
        "n_features = X_train.shape[2]\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(SEQ_LEN, n_features)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    LSTM(100, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Evaluate CNN-LSTM\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
        "y_test_orig = target_scaler.inverse_transform(y_test)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_orig, y_pred)\n",
        "mae = mean_absolute_error(y_test_orig, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"\\n📊 Hybrid CNN-LSTM Metrics\")\n",
        "print(f\"MSE  : {mse:.6f}\")\n",
        "print(f\"MAE  : {mae:.6f}\")\n",
        "print(f\"RMSE : {rmse:.6f}\")\n",
        "\n",
        "# Plot Actual vs Predicted\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(y_test_orig.flatten(), label='Actual Magnitude', color='orange', linewidth=1)\n",
        "plt.plot(y_pred.flatten(), label='Predicted Magnitude', color='blue', linewidth=1)\n",
        "plt.title('Hybrid CNN-LSTM: Actual vs Predicted (Test set)')\n",
        "plt.xlabel('Test sample index')\n",
        "plt.ylabel('Magnitude')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# Univariate LSTM\n",
        "def create_univariate_sequences(y_vec, seq_len=20):\n",
        "    Xu, yu = [], []\n",
        "    for i in range(len(y_vec) - seq_len):\n",
        "        Xu.append(y_vec[i:i+seq_len, 0])\n",
        "        yu.append(y_vec[i+seq_len, 0])\n",
        "    return np.array(Xu), np.array(yu)\n",
        "\n",
        "X_u, y_u = create_univariate_sequences(target_scaled, seq_len=SEQ_LEN)\n",
        "X_u = X_u.reshape((X_u.shape[0], X_u.shape[1], 1))\n",
        "y_u = y_u.reshape(-1,1)\n",
        "\n",
        "split_idx_u = int((1 - test_size) * len(X_u))\n",
        "X_u_train, X_u_test = X_u[:split_idx_u], X_u[split_idx_u:]\n",
        "y_u_train, y_u_test = y_u[:split_idx_u], y_u[split_idx_u:]\n",
        "\n",
        "uni_model = Sequential([\n",
        "    LSTM(64, input_shape=(SEQ_LEN,1)),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "uni_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "uni_model.summary()\n",
        "\n",
        "es2 = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "uni_history = uni_model.fit(X_u_train, y_u_train, validation_data=(X_u_test,y_u_test),\n",
        "                           epochs=40, batch_size=32, callbacks=[es2], verbose=2)\n",
        "\n",
        "# Evaluate univariate LSTM\n",
        "y_u_pred_scaled = uni_model.predict(X_u_test)\n",
        "y_u_pred = target_scaler.inverse_transform(y_u_pred_scaled)\n",
        "y_u_test_orig = target_scaler.inverse_transform(y_u_test)\n",
        "\n",
        "mse_u = mean_squared_error(y_u_test_orig, y_u_pred)\n",
        "mae_u = mean_absolute_error(y_u_test_orig, y_u_pred)\n",
        "rmse_u = np.sqrt(mse_u)\n",
        "\n",
        "print(f\"\\n📊 Univariate LSTM Metrics\")\n",
        "print(f\"MSE  : {mse_u:.6f}\")\n",
        "print(f\"MAE  : {mae_u:.6f}\")\n",
        "print(f\"RMSE : {rmse_u:.6f}\")\n",
        "\n",
        "# Forecast future\n",
        "def forecast_future_univariate(model, recent_scaled_mags, n_steps=10):\n",
        "    seq = recent_scaled_mags.copy().tolist()\n",
        "    preds = []\n",
        "    for _ in range(n_steps):\n",
        "        x = np.array(seq[-SEQ_LEN:]).reshape(1, SEQ_LEN, 1)\n",
        "        p = model.predict(x)[0,0]\n",
        "        preds.append(p)\n",
        "        seq.append(p)\n",
        "    return np.array(preds)\n",
        "\n",
        "recent_seq = target_scaled[-SEQ_LEN:, 0]\n",
        "N_FUTURE = 20\n",
        "future_preds_scaled = forecast_future_univariate(uni_model, recent_seq, n_steps=N_FUTURE)\n",
        "future_preds = target_scaler.inverse_transform(future_preds_scaled.reshape(-1,1)).flatten()\n",
        "\n",
        "print(f\"\\nNext {N_FUTURE} forecasted magnitudes (univariate LSTM):\")\n",
        "print(np.round(future_preds, 3))\n",
        "\n",
        "hist_mag = df[target_col].values[-200:]\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(range(len(hist_mag)), hist_mag, label='Recent historical magnitudes')\n",
        "plt.plot(range(len(hist_mag)-1, len(hist_mag)-1+N_FUTURE+1), np.concatenate([[hist_mag[-1]], future_preds]),\n",
        "         label='Forecasted magnitudes', marker='o')\n",
        "plt.title('Historical (last 200) and Forecasted Magnitudes')\n",
        "plt.xlabel('Index (time-ordered events)')\n",
        "plt.ylabel('Magnitude')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Done.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-26T16:27:47.80792Z",
          "iopub.execute_input": "2025-08-26T16:27:47.808318Z",
          "iopub.status.idle": "2025-08-26T16:33:04.591452Z",
          "shell.execute_reply.started": "2025-08-26T16:27:47.808292Z",
          "shell.execute_reply": "2025-08-26T16:33:04.590441Z"
        },
        "id": "mC_nsOi3doXN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#best algorithim is CNN-LSTM (as it got lowesr rmse score)\n",
        "# Model             MSE       MAE     RMSE\n",
        "Hybrid CNN-LSTM   0.0231    0.119   0.152\n",
        "Univariate LSTM   0.0315    0.143   0.177\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "NUhlruSZdoXO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-LSTM seismic forecasting with different epoch settings\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Load dataset\n",
        "csv_path = '/kaggle/input/earthquake-database/database.csv'\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# -------------------------\n",
        "# Create datetime if missing\n",
        "if 'Datetime' not in data.columns:\n",
        "    if 'Date' in data.columns and 'Time' in data.columns:\n",
        "        data['Datetime'] = pd.to_datetime(data['Date'].astype(str) + ' ' + data['Time'].astype(str), errors='coerce')\n",
        "    elif 'time' in data.columns:\n",
        "        data['Datetime'] = pd.to_datetime(data['time'], errors='coerce')\n",
        "    else:\n",
        "        data['Datetime'] = pd.to_datetime(pd.Series(range(len(data))), errors='coerce')\n",
        "\n",
        "# Select useful columns\n",
        "df = data[['Latitude','Longitude','Depth','Magnitude','Datetime']].dropna().sort_values('Datetime').reset_index(drop=True)\n",
        "\n",
        "# -------------------------\n",
        "# Scale data\n",
        "feature_cols = ['Latitude','Longitude','Depth']\n",
        "target_col = 'Magnitude'\n",
        "feature_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "\n",
        "features_scaled = feature_scaler.fit_transform(df[feature_cols].values.astype(np.float32))\n",
        "target_scaled = target_scaler.fit_transform(df[[target_col]].values.astype(np.float32))\n",
        "\n",
        "# -------------------------\n",
        "# Create sequences\n",
        "def create_sequences(X, y, seq_len=20):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - seq_len):\n",
        "        Xs.append(X[i:i+seq_len])\n",
        "        ys.append(y[i+seq_len])\n",
        "    return np.array(Xs), np.array(ys).reshape(-1,1)\n",
        "\n",
        "SEQ_LEN = 20\n",
        "X, y = create_sequences(features_scaled, target_scaled, SEQ_LEN)\n",
        "\n",
        "# Train-test split\n",
        "test_size = 0.2\n",
        "split_idx = int((1 - test_size) * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# -------------------------\n",
        "# Function to build CNN-LSTM\n",
        "def build_cnn_lstm(seq_len, n_features):\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(seq_len, n_features)),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        LSTM(100),\n",
        "        Dropout(0.3),\n",
        "        Dense(50, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# -------------------------\n",
        "# Train and evaluate for different epoch settings\n",
        "results = {}\n",
        "n_features = X_train.shape[2]\n",
        "\n",
        "for epochs in [40, 60, 70, 80]:\n",
        "    print(f\"\\nTraining CNN-LSTM with {epochs} epochs...\")\n",
        "    model = build_cnn_lstm(SEQ_LEN, n_features)\n",
        "    es = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=0)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=epochs,\n",
        "        batch_size=32,\n",
        "        callbacks=[es],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
        "    y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
        "    y_test_orig = target_scaler.inverse_transform(y_test)\n",
        "\n",
        "    # Metrics\n",
        "    mse = mean_squared_error(y_test_orig, y_pred)\n",
        "    mae = mean_absolute_error(y_test_orig, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    results[epochs] = {\"MSE\": mse, \"MAE\": mae, \"RMSE\": rmse}\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(y_test_orig.flatten(), label='Actual')\n",
        "    plt.plot(y_pred.flatten(), label=f'Predicted ({epochs} epochs)')\n",
        "    plt.title(f'CNN-LSTM (Epochs={epochs}) - Actual vs Predicted')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# Show results table\n",
        "print(\"\\nPerformance Comparison (CNN-LSTM):\")\n",
        "df_results = pd.DataFrame(results).T\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-26T17:05:37.799144Z",
          "iopub.execute_input": "2025-08-26T17:05:37.7995Z",
          "iopub.status.idle": "2025-08-26T17:09:45.944376Z",
          "shell.execute_reply.started": "2025-08-26T17:05:37.79947Z",
          "shell.execute_reply": "2025-08-26T17:09:45.94345Z"
        },
        "id": "vCBqNWbGdoXO"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}